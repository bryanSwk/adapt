{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./llama-2-7b-chat.ggmlv3.q5_K_M.bin\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    n_ctx= 2048,\n",
    "    n_batch=126,\n",
    "    callback_manager=callback_manager,\n",
    "    lora_path=\"ggml-adapter-model.bin\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"search for prompt engineering tips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful research assistant. The following functions are available for you to fetch further data to answer user questions, if relevant:\n",
    "\n",
    "{{\n",
    "    \"function\": \"search_google\",\n",
    "    \"description\": \"Search the web for content on Google. This allows users to search online/the internet/the web for content.\",\n",
    "    \"arguments\": [\n",
    "        {{\n",
    "            \"name\": \"query\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The search query string\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "{{\n",
    "    \"function\": \"search_arxiv\",\n",
    "    \"description\": \"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\",\n",
    "    \"arguments\": [\n",
    "        {{\n",
    "            \"name\": \"query\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The search query string\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "To call a function, respond - immediately and only - with a JSON object of the following format:\n",
    "{{\n",
    "    \"function\": \"function_name\",\n",
    "    \"arguments\": {{\n",
    "        \"argument1\": \"argument_value\",\n",
    "        \"argument2\": \"argument_value\"\n",
    "    }}\n",
    "}}\n",
    "<</SYS>>\n",
    "\n",
    "{input} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=0.5,\n",
    "    echo=False,\n",
    "    stop=[\"#\"],\n",
    "):\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    output_text = output\n",
    "    # [\"choices\"][0][\"text\"].strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"function\": \"search_google\",\n",
      "\"arguments\": {\n",
      "\"query\": \"prompt engineering tips\"\n",
      "}\n",
      "}"
     ]
    }
   ],
   "source": [
    "example = generate_text(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = json.loads(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#Search Libraries\n",
    "from googlesearch import search\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(query, num_results=5):\n",
    "    search_results = search(query, sleep_interval=5, num_results=num_results)\n",
    "    return list(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "    query = query,\n",
    "    max_results = max_results,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    url = []\n",
    "\n",
    "    for result in search.results():\n",
    "        print(result.title)\n",
    "        url.append(result.pdf_url)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_mapping = {\n",
    "    'search_google': search_google,\n",
    "    'search_arxiv': search_arxiv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_function = function_mapping.get(eg['function'], lambda *args, **kwargs: [])\n",
    "results = results_function(eg['arguments']['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://medium.com/@karankakwani/mastering-prompt-engineering-for-chatgpt-tips-tricks-and-best-practices-a2d01b703dab',\n",
       " 'https://colinscotland.com/unleash-the-power-of-chatgpt-11-epic-prompt-engineering-tips/',\n",
       " 'https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering',\n",
       " 'https://www.promptingguide.ai/introduction/tips',\n",
       " 'https://medicalfuturist.com/prompt-engineering-11-tips-to-craft-great-chatgpt-prompts/',\n",
       " 'https://docs.kanaries.net/articles/chatgpt-prompt-engineering',\n",
       " 'https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import BrowserlessLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\bark\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\anaconda3\\envs\\bark\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                 model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loader = TextLoader('./test.txt')\n",
    "# # documents = loader.load()\n",
    "# def summary(objective, text):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         separators=[\"\\n\\n\", \"\\n\"],\n",
    "#         chunk_size = 1000, \n",
    "#         chunk_overlap = 50,\n",
    "#         length_function = len)\n",
    "#     docs = text_splitter.create_documents(text)\n",
    "#     db = FAISS.from_documents(docs, embedder)\n",
    "#     # docs = db.similarity_search(objective)\n",
    "#     return db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['https://medium.com/@karankakwani/mastering-prompt-engineering-for-chatgpt-tips-tricks-and-best-practices-a2d01b703dab',\n",
    " 'https://colinscotland.com/unleash-the-power-of-chatgpt-11-epic-prompt-engineering-tips/',\n",
    " 'https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering',\n",
    " 'https://www.promptingguide.ai/introduction/tips',\n",
    " 'https://medicalfuturist.com/prompt-engineering-11-tips-to-craft-great-chatgpt-prompts/',\n",
    " 'https://docs.kanaries.net/articles/chatgpt-prompt-engineering',\n",
    " 'https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(url):\n",
    "    print(\"Start summary..\")\n",
    "    loader = BrowserlessLoader(api_token='d15ecd5b-c5cc-47e0-a8c9-872ba903097c', urls=url)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    all_splits = text_splitter.split_documents(documents)       \n",
    "    db = FAISS.from_documents(all_splits, embedder)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summary..\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\site-packages\\requests\\models.py:960\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 960\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39;49mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(encoding), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    961\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[0;32m    963\u001b[0m     \u001b[39m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[0;32m    964\u001b[0m     \u001b[39m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[0;32m    965\u001b[0m     \u001b[39m# used.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m summary(results[:\u001b[39m2\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart summary..\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m loader \u001b[39m=\u001b[39m BrowserlessLoader(api_token\u001b[39m=\u001b[39mbrowserless_api_key, urls\u001b[39m=\u001b[39murl)\n\u001b[1;32m----> 4\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m      5\u001b[0m text_splitter \u001b[39m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m      6\u001b[0m all_splits \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39msplit_documents(documents)       \n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\site-packages\\langchain\\document_loaders\\browserless.py:67\u001b[0m, in \u001b[0;36mBrowserlessLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load Documents from URLs.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy_load())\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\site-packages\\langchain\\document_loaders\\browserless.py:42\u001b[0m, in \u001b[0;36mBrowserlessLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_content:\n\u001b[0;32m     27\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[0;32m     28\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://chrome.browserless.io/scrape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m         params\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         },\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m     \u001b[39myield\u001b[39;00m Document(\n\u001b[1;32m---> 42\u001b[0m         page_content\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39;49mjson()[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     43\u001b[0m         metadata\u001b[39m=\u001b[39m{\n\u001b[0;32m     44\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: url,\n\u001b[0;32m     45\u001b[0m         },\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[0;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://chrome.browserless.io/content\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m         params\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m         },\n\u001b[0;32m     56\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\bark\\lib\\site-packages\\requests\\models.py:968\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    967\u001b[0m         \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 968\u001b[0m             \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n\u001b[0;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "result = summary(results[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = result.similarity_search(\"prompt engineering\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI marketing coach, I’ve spent countless hours learning how to manipulate language to get outstanding results from this powerful technology. And the best part? You don’t need to be a coder or tech-savvy to succeed with ChatGPT.\n",
      "\n",
      "Prompt engineering is all about the text that you put into the ChatGPT text interface. By learning the basics of prompt engineering, you’ll be in the top 1% of users in the world in terms of your ability to get the best out of this platform.\n",
      "\n",
      "So, let’s dive into 11 ways to level up your game when it comes to prompt engineering in ChatGPT:\n",
      "Mastering Prompt Engineering for ChatGPT: Tips, Tricks, and Best Practices\n",
      "Unlock the full potential of ChatGPT with these expert strategies for crafting effective prompts.\n",
      "\n",
      "Karan Kakwani\n",
      "\n",
      "·\n",
      "\n",
      "Follow\n",
      "\n",
      "6 min read\n",
      "·\n",
      "Apr 30\n",
      "\n",
      "--\n",
      "\n",
      "2\n",
      "\n",
      "ChatGPT, the groundbreaking language model developed by OpenAI, has revolutionized the way we interact with AI-powered systems. As a powerful tool for generating human-like text, ChatGPT has a wide range of applications, from content creation to customer support. However, to truly harness its potential, it’s essential to master the art of prompt engineering. In this article, we’ll explore the best practices, tips, and tricks for crafting effective prompts that will help you get the most out of ChatGPT, along with real-world examples to illustrate their effectiveness.\n",
      "\n",
      "Understanding the Basics of ChatGPT\n",
      "In conclusion, prompt engineering is the key to getting the best possible results from ChatGPT. By implementing these 11 tips, you will be able to engineer prompts that get infinitely better responses from ChatGPT. So, go ahead and give them a try and take your prompt engineering skills to the next level. Happy prompting!\n",
      "\n",
      "Don’t forget to check out my YouTube video where I dive deeper into these tips and provide examples to help you get started. And if you found this post helpful, please share it with others who might find it useful. Thanks \n",
      "\n",
      "PREVIOUS\n",
      "NEXT\n",
      "ChatGPT for Beginners: A Quick Start Guide for Entrepreneurs\n",
      "ChatGPT for Entrepreneurs: Unleash the Power of AI in Your Business\n",
      "Privacy\n",
      "Terms\n",
      "Disclaimer\n",
      "\n",
      "Copyright © Colin Scotland. All rights reserved.\n",
      "Be Clear and Specific\n",
      "The first and most important tip for prompt engineering is to be clear and specific. When crafting your prompts, make sure they are unambiguous and clearly specify the desired format, tone, and output you want from ChatGPT. For instance, instead of asking ChatGPT to “write a blog post on productivity,” try asking it to “write an informative blog post giving three productivity tips for remote work.” By providing clear and specific prompts, you can get more accurate and relevant responses from ChatGPT.\n",
      "Provide Context\n",
      "Before diving into prompt engineering, it’s crucial to understand how ChatGPT works. As a transformer-based model, ChatGPT is trained on vast amounts of text data and can generate contextually relevant responses based on the input it receives. By providing a well-crafted prompt, you can guide the model to generate the desired output.\n",
      "\n",
      "Prompt Engineering\n",
      "\n",
      "1. Be Clear and Specific\n",
      "\n",
      "When crafting prompts, clarity and specificity are key. Make sure your prompt is concise and clearly communicates the information you want from ChatGPT. Avoid ambiguous language and provide enough context to help the model understand your request.\n",
      "\n",
      "Example:\n",
      "\n",
      "Less effective: “Tell me about AI.”\n",
      "More effective: “Explain the key principles of artificial intelligence and its applications in healthcare.”\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.com/@karankakwani/mastering-prompt-engineering-for-chatgpt-tips-tricks-and-best-practices-a2d01b703dab'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.similarity_search(\"prompt\", 2)[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, result.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n",
      "Unhelpful Answer: Sure, here are some tips for prompt engineering: 1) be clear and specific, 2) test and iterate, 3) phrase your questions carefully, and 4) anchor your prompts with examples. I don't know.\n",
      "Unhelpful Answer: Sure, here are some tips for prompt engineering: 1) be clear and specific, 2) test and iterate, 3) phrase your questions carefully, and 4) anchor your prompts with examples.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"how to prompt engineer?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can I clearly and specifically define my prompts to engineer the desired response?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To clearly and specifically define your prompts, you should aim to be as detailed and specific as possible. This means providing enough context for ChatGPT to understand what you want it to generate, while also avoiding ambiguity or confusion. For example, instead of asking \"Write a story,\" try asking \"Write a 500-word story about a character who learns a valuable lesson after experiencing failure.\" By providing specific details and requirements, you can increase the likelihood of getting the desired response from ChatGPT.  To clearly and specifically define your prompts, you should aim to be as detailed and specific as possible. This means providing enough context for ChatGPT to understand what you want it to generate, while also avoiding ambiguity or confusion. For example, instead of asking \"Write a story,\" try asking \"Write a 500-word story about a character who learns a valuable lesson after experiencing failure.\" By providing specific details and requirements, you can increase the likelihood of getting the desired response from ChatGPT.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"Summarise\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[329], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m      3\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat are some helpful tips?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m retriever \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49msimilarity_search(question)\u001b[39m.\u001b[39;49mas_retriever(return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(llm\u001b[39m=\u001b[39mllm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, retriever\u001b[39m=\u001b[39mretriever)\n\u001b[0;32m      6\u001b[0m qa\u001b[39m.\u001b[39mrun(question)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question = \"What are some helpful tips?\"\n",
    "retriever = result.similarity_search(question).as_retriever(return_source_documents=True)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know the answer to that question as it is not provided in the given context."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I don't know the answer to that question as it is not provided in the given context.\""
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is his occupation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
