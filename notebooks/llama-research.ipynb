{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./llama-2-7b-chat.ggmlv3.q5_K_M.bin\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    n_ctx= 2048,\n",
    "    n_batch=126,\n",
    "    callback_manager=callback_manager,\n",
    "    lora_path=\"ggml-adapter-model.bin\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"search for prompt engineering tips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful research assistant. The following functions are available for you to fetch further data to answer user questions, if relevant:\n",
    "\n",
    "{{\n",
    "    \"function\": \"search_google\",\n",
    "    \"description\": \"Search the web for content on Google. This allows users to search online/the internet/the web for content.\",\n",
    "    \"arguments\": [\n",
    "        {{\n",
    "            \"name\": \"query\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The search query string\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "{{\n",
    "    \"function\": \"search_arxiv\",\n",
    "    \"description\": \"Search for research papers on ArXiv. Make use of AND, OR and NOT operators as appropriate to join terms within the query.\",\n",
    "    \"arguments\": [\n",
    "        {{\n",
    "            \"name\": \"query\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The search query string\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "To call a function, respond - immediately and only - with a JSON object of the following format:\n",
    "{{\n",
    "    \"function\": \"function_name\",\n",
    "    \"arguments\": {{\n",
    "        \"argument1\": \"argument_value\",\n",
    "        \"argument2\": \"argument_value\"\n",
    "    }}\n",
    "}}\n",
    "<</SYS>>\n",
    "\n",
    "{input} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=0.5,\n",
    "    echo=False,\n",
    "    stop=[\"#\"],\n",
    "):\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    output_text = output\n",
    "    # [\"choices\"][0][\"text\"].strip()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"function\": \"search_google\",\n",
      "\"arguments\": {\n",
      "\"query\": \"prompt engineering tips\"\n",
      "}\n",
      "}"
     ]
    }
   ],
   "source": [
    "example = generate_text(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = json.loads(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#Search Libraries\n",
    "from googlesearch import search\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(query, num_results=5):\n",
    "    search_results = search(query, sleep_interval=5, num_results=num_results)\n",
    "    return list(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "    query = query,\n",
    "    max_results = max_results,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    url = []\n",
    "\n",
    "    for result in search.results():\n",
    "        print(result.title)\n",
    "        url.append(result.pdf_url)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_mapping = {\n",
    "    'search_google': search_google,\n",
    "    'search_arxiv': search_arxiv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_function = function_mapping.get(eg['function'], lambda *args, **kwargs: [])\n",
    "results = results_function(eg['arguments']['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://medium.com/@karankakwani/mastering-prompt-engineering-for-chatgpt-tips-tricks-and-best-practices-a2d01b703dab',\n",
       " 'https://colinscotland.com/unleash-the-power-of-chatgpt-11-epic-prompt-engineering-tips/',\n",
       " 'https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering',\n",
       " 'https://www.promptingguide.ai/introduction/tips',\n",
       " 'https://medicalfuturist.com/prompt-engineering-11-tips-to-craft-great-chatgpt-prompts/',\n",
       " 'https://docs.kanaries.net/articles/chatgpt-prompt-engineering',\n",
       " 'https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import BrowserlessLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                 model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loader = TextLoader('./test.txt')\n",
    "# # documents = loader.load()\n",
    "# def summary(objective, text):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         separators=[\"\\n\\n\", \"\\n\"],\n",
    "#         chunk_size = 1000, \n",
    "#         chunk_overlap = 50,\n",
    "#         length_function = len)\n",
    "#     docs = text_splitter.create_documents(text)\n",
    "#     db = FAISS.from_documents(docs, embedder)\n",
    "#     # docs = db.similarity_search(objective)\n",
    "#     return db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(url):\n",
    "    print(\"Start summary..\")\n",
    "    loader = BrowserlessLoader(api_token='d15ecd5b-c5cc-47e0-a8c9-872ba903097c', urls=url)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    all_splits = text_splitter.split_documents(documents)       \n",
    "    db = FAISS.from_documents(all_splits, embedder)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start summary:\n"
     ]
    }
   ],
   "source": [
    "result = summary(results[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, result.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n",
      "Unhelpful Answer: Sure, here are some tips for prompt engineering: 1) be clear and specific, 2) test and iterate, 3) phrase your questions carefully, and 4) anchor your prompts with examples. I don't know.\n",
      "Unhelpful Answer: Sure, here are some tips for prompt engineering: 1) be clear and specific, 2) test and iterate, 3) phrase your questions carefully, and 4) anchor your prompts with examples.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"how to prompt engineer?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can I clearly and specifically define my prompts to engineer the desired response?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  To clearly and specifically define your prompts, you should aim to be as detailed and specific as possible. This means providing enough context for ChatGPT to understand what you want it to generate, while also avoiding ambiguity or confusion. For example, instead of asking \"Write a story,\" try asking \"Write a 500-word story about a character who learns a valuable lesson after experiencing failure.\" By providing specific details and requirements, you can increase the likelihood of getting the desired response from ChatGPT.  To clearly and specifically define your prompts, you should aim to be as detailed and specific as possible. This means providing enough context for ChatGPT to understand what you want it to generate, while also avoiding ambiguity or confusion. For example, instead of asking \"Write a story,\" try asking \"Write a 500-word story about a character who learns a valuable lesson after experiencing failure.\" By providing specific details and requirements, you can increase the likelihood of getting the desired response from ChatGPT.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"Summarise\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[329], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m      3\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat are some helpful tips?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m retriever \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49msimilarity_search(question)\u001b[39m.\u001b[39;49mas_retriever(return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(llm\u001b[39m=\u001b[39mllm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, retriever\u001b[39m=\u001b[39mretriever)\n\u001b[0;32m      6\u001b[0m qa\u001b[39m.\u001b[39mrun(question)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question = \"What are some helpful tips?\"\n",
    "retriever = result.similarity_search(question).as_retriever(return_source_documents=True)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know the answer to that question as it is not provided in the given context."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I don't know the answer to that question as it is not provided in the given context.\""
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is his occupation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
